{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import glob\n",
    "from shutil import copyfile, copy2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists and dictionaries that help make translating between class labels and mask colors easier\n",
    "LABELS = ['Acrop', 'CCA', 'Macro', 'Monti', 'Off', 'Pavon', 'Pocill', 'Porit', 'Sand', 'Turf']\n",
    "\n",
    "labels = {'Acrop' : 0,\n",
    "          'CCA' : 1,\n",
    "          'Macro' : 2,\n",
    "          'Monti' : 3,\n",
    "          'Off' : 4,\n",
    "          'Pavon' : 5,\n",
    "          'Pocill': 6,\n",
    "          'Porit' : 7,\n",
    "          'Sand' : 8,\n",
    "          'Turf' : 9}\n",
    "\n",
    "EXP_LABELS = ['Acrop', 'CCA', 'Macro', 'Monti', 'Pavon', 'Pocill', 'Porit', 'Sand', 'Turf']\n",
    "\n",
    "NO_LABEL = 255\n",
    "\n",
    "\n",
    "old_labels = pd.read_csv(\"Replace_With.csv\")\n",
    "old_labels = dict(zip(old_labels['Old'], old_labels['New']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = glob.glob(\"Data\\\\2008\\\\images\\\\*.jpg\")\n",
    "labs = glob.glob(\"Data\\\\2008\\\\cpce\\\\*.txt\")\n",
    "\n",
    "data = pd.DataFrame(data = list(zip(imgs, labs)), columns = ['images', 'cpce'])\n",
    "\n",
    "dest = \"Data\\\\2008\\\\patches_224\"\n",
    "\n",
    "for l in LABELS:\n",
    "    if(not os.path.exists(dest + \"\\\\\" + l)):\n",
    "        os.mkdir(dest + \"\\\\\" + l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_patch(image, y, x):\n",
    "\n",
    "    size = 112\n",
    "    \n",
    "    patch = image[abs(size - y) : abs(size + y), abs(size - x) : abs(size + x), :]\n",
    "    \n",
    "    return patch\n",
    "\n",
    "\n",
    "def check_dimensions(image, y, x):\n",
    "    \n",
    "    size = 224\n",
    "    \n",
    "    height, width = image.shape[0:2]\n",
    "    if(x + (size//2) > width or x - (size//2) < 0 or y + (size//2) > height or y - (size//2) < 0):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "    \n",
    "def extract_patches(data):\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        each_image = io.imread(data['images'][i])\n",
    "        each_annotation = pd.read_csv(data['cpce'][i], sep = \"; \", \n",
    "                                      engine = 'python').rename(columns={'# Row' : 'Y', 'Col': 'X'})\n",
    "        \n",
    "\n",
    "        each_annotation.replace(old_labels, inplace = True)   \n",
    "\n",
    "        file_name = (data['cpce'][i].split(\"\\\\\")[-1].split(\".\")[0]);\n",
    "\n",
    "        height, width = each_image.shape[0:2]\n",
    "        \n",
    "        for index, row in each_annotation.iterrows():\n",
    "            \n",
    "            X = int(row[1])\n",
    "            Y = int(row[0])\n",
    "            L = str(row[2])\n",
    "        \n",
    "            \n",
    "            if(L not in LABELS):\n",
    "                continue\n",
    "            \n",
    "            if(check_dimensions(each_image, Y, X)):\n",
    "                patch = crop_patch(each_image, Y, X)\n",
    "              \n",
    "            else:\n",
    "                continue      \n",
    "\n",
    "            io.imsave(arr = patch, fname = dest + \"\\\\\" + L +  \"\\\\\" + file_name + \"_\" + str(index) + \".png\")\n",
    "            \n",
    "    print(\"Complete\")       \n",
    "    \n",
    "#extract_patches(data) # <-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = glob.glob(\"Data\\\\2008\\\\patches_224\\\\**\\\\*.png\", recursive = True)\n",
    "labels = [patch.split(\"\\\\\")[-2] for patch in patches]\n",
    "\n",
    "data = pd.DataFrame(data = list(zip(patches, labels)), columns = ['images', 'labels'])\n",
    "data = data[data['labels'] != 'Off']\n",
    "\n",
    "train, valid = train_test_split(data, test_size = .125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 112814 Validation: 16117 Testing: 125888\n"
     ]
    }
   ],
   "source": [
    "# perform the split between training and validation\n",
    "\n",
    "patches = glob.glob(\"Data\\\\2009\\\\patches_224\\\\**\\\\*.png\", recursive = True)\n",
    "labels = [patch.split(\"\\\\\")[-2] for patch in patches]\n",
    "\n",
    "test = pd.DataFrame(data = list(zip(patches, labels)), columns = ['images', 'labels'])\n",
    "test = test[test['labels'] != 'Off']\n",
    "\n",
    "print(\"Training:\", len(train), \"Validation:\", len(valid), \"Testing:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 112814 images belonging to 9 classes.\n",
      "Found 16117 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "from panel_image import *\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Training images are augmented, and then lightly pre-processed\n",
    "train_augmentor = ImageDataGenerator(preprocessing_function = None, \n",
    "                                            horizontal_flip = True, \n",
    "                                            vertical_flip = True,\n",
    "                                            rescale = 1.0/255.0)      \n",
    "                                     \n",
    "                                                                   \n",
    "# Reading from dataframe, can save augmented images if needed\n",
    "train_generator = train_augmentor.flow_from_dataframe(dataframe = train, directory = None,\n",
    "                                                      x_col = 'images', y_col = 'labels', target_size = (224, 224), \n",
    "                                                      color_mode = \"rgb\",  class_mode = 'categorical', \n",
    "                                                      batch_size = batch_size, shuffle = True, seed = 42)\n",
    "                                                     \n",
    "\n",
    "# Only pre-process images, no augmentation\n",
    "validate_augmentor = ImageDataGenerator( preprocessing_function = None,\n",
    "                                         rescale = 1.0/255.0)\n",
    "\n",
    "\n",
    "# Reading from dataframe                             \n",
    "validation_generator = validate_augmentor.flow_from_dataframe(dataframe = valid, directory = None, \n",
    "                                                              x_col = 'images', y_col = 'labels', target_size = (224, 224), \n",
    "                                                              color_mode = \"rgb\",  class_mode = 'categorical', \n",
    "                                                              batch_size = batch_size, shuffle = True, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "\n",
    "# Defines the length of an epoch, all images used\n",
    "steps_per_epoch_train = len(train)/batch_size\n",
    "\n",
    "# Defines the length of an epoch, all images used\n",
    "steps_per_epoch_valid = len(valid)/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transfer-learning model\n",
    "from keras.models import Model\n",
    "from keras.applications.nasnet import NASNetMobile\n",
    "from keras.layers import Input, Dense, BatchNormalization, Activation, Dropout\n",
    "\n",
    "main_input = Input(shape = (224, 224, 3))\n",
    "base = NASNetMobile(include_top = False, weights = 'imagenet', pooling = 'max')(main_input)\n",
    "x = Dropout(.75)(base)\n",
    "x = Dense(9)(x)\n",
    "main_output = Activation('softmax')(x)\n",
    "\n",
    "model = Model(inputs = [main_input], outputs = [main_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 0.00025\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = optimizers.Adam(lr = learning_rate), \n",
    "              metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "holla = [\n",
    "         LearningRateScheduler(reduceLR),\n",
    "         ModelCheckpoint(filepath = \"Experiment_1\\\\Panel.h5\", \n",
    "                         monitor = 'val_loss', \n",
    "                         save_weights_only = True, \n",
    "                         save_best_only = True, verbose = 1)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts training, saves info for later\n",
    "history = model.fit_generator(train_generator, \n",
    "                              steps_per_epoch = steps_per_epoch_train, \n",
    "                              epochs = num_epochs, \n",
    "                              validation_data = validation_generator, \n",
    "                              validation_steps = steps_per_epoch_valid,\n",
    "                              callbacks = holla,\n",
    "                              verbose = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (15, 10))\n",
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(history.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(history.history[\"val_acc\"], label=\"val_acc\")\n",
    "\n",
    "plt.plot(np.argmin(history.history[\"val_loss\"]), \n",
    "         np.min(history.history[\"val_loss\"]), \n",
    "         marker = \"x\", color = \"b\", label = \"best model\")\n",
    "\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"Experiment_1\\\\Panel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads from dataframe for holdout set\n",
    "test_generator = validate_augmentor.flow_from_dataframe(dataframe = test,\n",
    "                                                 x_col = 'images', y_col = 'labels', target_size = (224, 224), \n",
    "                                                 color_mode = \"rgb\",  class_mode = 'categorical', \n",
    "                                                 batch_size = batch_size, shuffle = False, seed = 42)\n",
    "# Defines the length of an epoch\n",
    "steps_per_epoch_test = len(test)/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "predictions = model.predict_generator(test_generator, steps = steps_per_epoch_test)\n",
    "predict_classes = np.argmax(predictions, axis = 1)\n",
    "\n",
    "test_y = test_generator.classes\n",
    "print(\"# of images:\", len(predict_classes))\n",
    "print(accuracy_score(y_true = test_y, y_pred = predict_classes))\n",
    "print(confusion_matrix(y_true = test_y, y_pred = predict_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_points(image, percent, method):\n",
    " \n",
    "    num_points = int((image.shape[0] * image.shape[1]) * percent)\n",
    "    \n",
    "    offset = 200 # about 7.5%\n",
    "    \n",
    "    if(method == 'grid'):\n",
    "    \n",
    "        density = int(np.sqrt(num_points)) \n",
    "\n",
    "        x_, y_ = np.meshgrid(np.linspace(offset, image.shape[1] - offset, density), \n",
    "                             np.linspace(offset, image.shape[0] - offset, density))\n",
    "\n",
    "        xy = np.dstack([x_, y_]).reshape(-1, 2).astype(int)\n",
    "        x = [point[0] for point in xy]\n",
    "        y = [point[1] for point in xy]\n",
    "        \n",
    "    elif(method == 'random'):\n",
    "        \n",
    "        x = np.random.randint(offset, image.shape[1] - offset, num_points)\n",
    "        y = np.random.randint(offset, image.shape[0] - offset, num_points)\n",
    "    else:\n",
    "        print(\"Choose a method for sampling sparse points.\")\n",
    "        return\n",
    "        \n",
    "    \n",
    "    patches = []\n",
    "    \n",
    "    for _ in range(len(x)):\n",
    "        \n",
    "        if(check_dimensions(image, y[_], x[_])):\n",
    "            patch = crop_patch(image, y[_], x[_])\n",
    "            patch = patch * (1.0/255.0)\n",
    "            patch = iaa.Resize(112).augment_image(patch)\n",
    "            patches.append(patch)\n",
    "            \n",
    "    sparse_predictions = model.predict(np.array(patches)).squeeze()\n",
    "\n",
    "    \n",
    "    labels_ = [list(EXP_LABELS)[np.argmax(s)] for s in sparse_predictions]\n",
    "    \n",
    "    confidence_ = [(sorted(s)[-1] - sorted(s)[-2]) for s in sparse_predictions]\n",
    "\n",
    "    df_sparse = pd.DataFrame(data = list(zip(y, x, labels_, confidence_)), columns = ['Y', 'X', 'Label', 'Confidence'])\n",
    "    \n",
    "    return df_sparse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
